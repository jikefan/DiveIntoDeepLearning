{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c354b1",
   "metadata": {},
   "source": [
    "# 自动微分简单例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6797b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 11:07:34.103563: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-09-11 11:07:34.104371: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-11 11:07:34.107323: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.range(4, dtype=tf.float32)\n",
    "\n",
    "x = tf.Variable(x)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "x_grad = t.gradient(y, x)\n",
    "# t.gradient(y, x)对在with tf.GradientTape() as t:中的计算过程进行求导\n",
    "y\n",
    "x\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4bb501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分离计算\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = x * x\n",
    "    u = tf.stop_gradient(y)\n",
    "    z = u * x\n",
    "\n",
    "x_grad = t.gradient(z, x)\n",
    "x_grad == u\n",
    "\n",
    "t.gradient(y, x) == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f2055a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = tf.Variable(tf.random.normal(shape=()))\n",
    "a\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    d = f(a)\n",
    "\n",
    "d_grad = t.gradient(d, a)\n",
    "d_grad\n",
    "\n",
    "d_grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf889bf",
   "metadata": {},
   "source": [
    "现在可以分析上面定义的f函数。 请注意，它在其输入a中是分段线性的。 换言之，对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a，因此可以用d/a验证梯度是否正确。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
